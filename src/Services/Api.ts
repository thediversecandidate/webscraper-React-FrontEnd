import axios, { AxiosResponse } from "axios"

const baseUrl: string = "http://34.234.193.247"


export const getArticles = async (search: string, maxResult: number): Promise<AxiosResponse<GetArticlesResponse>> => {
    axios.defaults.withCredentials = true;
    try {
        const results: AxiosResponse<GetArticlesResponse> = await axios.get(baseUrl + `/articles/search/${search}/${maxResult}`,
            {
                method: "OPTIONS",
                headers: {
                    "Authorization": "Token fd314d4436dfdc9fd990822cd1e483d951c7dfd6",
                    "Accept": "application/json",
                    "Access-Control-Allow-Origin": "*",
                    "Access-Control-Allow-Methods": "POST,GET,OPTIONS, PUT, DELETE"
                }
            });
        return results;
    } catch (error) {
        throw new Error(error)
    }
}

export const TEMP_ARTICLES: ArticleRow[] = [
    {
        url: "https://www.datacenterknowledge.com/archives/2013/02/11/video-installing-a-2-megawatt-generator",
        title: "Video: Installing a 2-Megawatt Generator",
        body: "SoftLayer Technologies recently expanded the power capacity of its DAL05 data center by installing a new Cummins 2-megawatt diesel backup generator. This 4-minute video reveals the process involved in lifting this enormous engine and installing it on a cement pad alongside the data center. \"You see the crane prepare for the work by installing counter-balance weights, and work starts with the team placing a utility transformer on its pad outside our generator yard,\" writes Robert Adkins, SoftLayer's VP of Facilities, in a blog post. \"A truck pulls up with the generator base in tow, and you watch the base get positioned and lowered into place. The base looks so large because it also serves as the generator’s 4,000 gallon “belly” fuel tank. After the base is installed, the generator is trucked in, and it is delicately picked up, moved, lined up and lowered onto its base. The last step you see is the generator housing being installed over the generator to protect it from the elements.\"\n\nFor a look at what goes on inside DAL05, see this video tour: Inside A SoftLayer Data Center. For more news about SoftLayer, see our SoftLayer Channel. For additional video, check out our DCK video archive and the Data Center Videos channel on YouTube.",
        article_summary: "SoftLayer Technologies recently expanded the power capacity of its DAL05 data center by installing a new Cummins 2-megawatt diesel backup generator. This 4-minute video reveals the process involved in lifting and installing this enormous engine.",
        list_of_keywords: "",
        wordcloud_words: "generator SoftLayer base video see installing For DAL05 data center pad work lowered The installed Data Center Technologies recently expanded power capacity new Cummins megawatt",
        wordcloud_scores: "7 5 5 4 4 3 3 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1",
        created_date: "2020-06-17"
    },
    {
        url: "https://www.datacenterknowledge.com/archives/2010/03/08/generators-installed-at-yahoo-ny-site",
        title: "Generators Installed at Yahoo NY Site",
        body: "There's just something about generators. They're the huge engines providing the last line of defense for the data center and the thousands of web sites inside. They're big, loud and powerful, and attract lots of attention during tours and installations.\n\nAnd sometimes, they serve as a sign of bigger things to come. Last Friday the backup generators arrived at the new Yahoo data center in Lockport, New York. Three 2-megawatt gensets were installed, generating coverage in The Buffalo News.\n\n\"The generators, measuring 11 by 46 feet, were lifted off flatbed trucks by a crane supplied by Clark Rigging and Hauling of Lockport,\" The News reported. \"They were placed atop 7,000-gallon diesel fuel tanks. (Cummins sales engineer Jay) Deshpande said one generator, one tank and a full load of fuel weigh a total of 170,000 pounds. Deshpande said an emergency generator will burn 143 gallons of diesel fuel in an hour. The power is produced by 60-liter, 2,500-horsepower engines.\"\n\nYahoo is building a $150 million data center on a 30-acre property in Lockport, which is expected to  create 125 local jobs paying $65,000 to $75,000 a year once its two-phase construction plan is completed. The first phase of construction is scheduled to be completed in late spring or early summer.\n\nWhat does a generator installation look like? Here are a couple of videos that can give you an idea:\n\nData Cave Installs 2 Megawatt Generator\nInstalling a 2.25 Megawatt Diesel Generator",
        article_summary: "Diesel generators are big and powerful, and attract lots of attention during tours and installations. Last Friday three 2 megawatt generators were installed at the new Yahoo data center under construction in Lockport, New York.",
        list_of_keywords: "",
        wordcloud_words: "The 000 generators They data center Lockport fuel generator engines Yahoo News diesel Deshpande said one phase construction completed Megawatt Generator There something huge providing",
        wordcloud_scores: "5 4 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1",
        created_date: "2020-06-17"
    },
    {
        url: "https://www.datacenterknowledge.com/archives/2015/09/30/six-facts-in-high-availability-data-center-design",
        title: "Six Facts in High-Availability Data Center Design",
        body: "As the data center increasingly becomes the heart of the enterprise, data center reliability needs increase. But data center design isn’t simply about infrastructure redundancy. As senior company executives pay more attention to what’s happening in the data center, it is more important than ever for a data center design to match specific company needs.\n\nMore redundancy than necessary means overspending and, as you’ll learn later in the article, can actually works against reliability. Steven Shapiro, mission critical practice lead at Morrison Hershfield, an engineering firm that does a lot of data center projects, said companies have to align business mission with expectations of data center performance when deciding how redundant the design should be.\n\nShapiro talked about the basics of data center design decisions from the availability perspective in a presentation at last week’s Data Center World conference in National Harbor, Maryland. Here are some of the highlights from his presentation:\n\nMore redundancy doesn’t always mean more reliability\n\nNot only is it important to design as much as possible for actual reliability needs of the applications, more infrastructure redundancy doesn’t automatically make a system more reliable. In fact, there is a point at which increasing component redundancy lowers reliability because the system becomes more complex and difficult to manage, Shapiro said.\n\nTier IV costs twice as much as Tier II\n\nInfrastructure reliability level has to match the needs of the applications the data center is supporting. Simply designing and building the most reliable data center you can afford is not the smart way to go, especially considering the cost of redundancy.\n\nThe difference in cost between an Uptime Institute Tier I and a Tier II design or between a Tier III and a Tier IV one is small, the jump from Tier II to Tier III is enormous: almost 100 percent. Citing Uptime’s own estimates, Shapiro said a Tier I data center with 15,000 square feet of computer space would cost $10,000 per kW of usable UPS-backed power capacity. The cost goes up to $11,000 for a Tier II facility, but to $20,000 for Tier III and $22,000 for Tier IV.\n\n2(N+1) UPS config not much more reliable than 2N UPS\n\nIn another example where more redundancy doesn’t mean more reliability, Shapiro said a design doesn’t get much more reliable by going from a 2N UPS configuration, which has enough UPS modules for the IT load times two, to a 2(N+1) configuration, which has IT load plus one more module times two.\n\nThe probability of failure for a system that has 2N UPS, N+1 generator capacity, dual utility feeds, an alternate-source transfer switch, and IT gear with dual power cords is 4.41 percent, according to Shapiro. A system that is the same in every other respect but has a UPS configuration of 2(N+1) has the same probability of failure.\n\n2N generator config is marginally more reliable than N+1\n\nA 2(N+1) generator configuration makes a difference in availability compared to an N+1 config, albeit a small one. In a system with 2(N+1) UPS, dual utility feeds, an alternate-source transfer switch, and dual-corded IT equipment, the difference in failure probability between an N+1 generator configuration and a 2(N+1) configuration is about 1.5 percent – 4.41 percent for the former and 2.94 percent for the latter.\n\nSatisfying even the highest Tier IV requirement in Uptime’s rating system doesn’t require prime rated generators. A standby rating is enough. Uptime’s requirements call for a generator that will run continuously, even during maintenance. That’s a guarantee all major generator manufacturers will readily provide, satisfying the requirement, Shapiro said.\n\nTier III and Tier IV requirements do, however, call for redundant power distribution from the generator plant and for the fuel supply infrastructure to be concurrently maintainable or fault-tolerant.\n\n15 percent of generators fail after eight hours of running\n\nGenerator redundancy is important because generators aren’t infallible. Even if a generator starts successfully, and the facility switches to backup power without incident, things change when generators have to run for prolonged periods of time.\n\nHurricane Sandy’s aftermath in New York provided that rare test of generator reliability when running at length, and many did fail the test. A number of facilities operated by Morrison Hershfield clients switched to generator power and saw the lights go down after hours of operation, Shapiro said. The failures happened for different reasons, but in one case, a genset failed when it reached the bottom of the fuel tank and took in impurities that had accumulated there and failed to filter out.\n\nHe cited a study by the Idaho National Engineering laboratory that found that 15 percent of emergency diesel generators failed after eight hours of continuous operation; one percent failed after 24 hours; five percent failed after half an hour; and 2 percent failed to start.\n\nTier requirements alone won’t determine reliability\n\nWhile Uptime’s Tier system defines reliability of infrastructure design, there are many factors that affect reliability beyond design. They include site location, construction of the building, quality of the equipment, the commissioning process, age of the site, operations and maintenance practices of the management, personnel training and level of personnel coverage.\n\nCorrected: A previous version of the article erroneously said Uptime's Tier IV requirements did not call for redundant generators. They do, and the article has been corrected accordingly. Tier IV doesn't require prime rated generators; a standby rating is satisfactory.",
        article_summary: "A more redundant design doesn’t always mean a more reliable one",
        list_of_keywords: "",
        wordcloud_words: "Tier data center reliability percent generator design redundancy Shapiro UPS said system generators doesn Uptime configuration failed reliable one 000 power needs infrastructure much cost",
        wordcloud_scores: "19 11 11 11 10 10 9 8 8 8 7 7 7 6 6 6 6 5 5 5 5 4 4 4 4",
        created_date: "2020-06-17"
    },
    {
        url: "https://www.datacenterknowledge.com/archives/2008/11/26/feel-the-power-eaton-active-power-peak-10",
        title: "Feel The Power: Eaton, Active Power, Peak 10",
        body: "We're feeling the power at DCK this morning, as there's been a flurry of power-related news. Here's a roundup:\n\nEaton Corporation has launched Eaton Enterprise Power Manager (EPM) software and a full line of power cables to help measure and manage power at the rack and server levels. EPM software aggregates power management information for Eaton's enclosure power distribution units (ePDUs) and midrange UPS units. “With the EPM software, you don't have to surf hundreds of URL addresses to gain important visibility into your enclosure power distribution statistics,” said Michael Camesano, product manager, ePDUs and enclosures, Eaton. “If you do not have a high-end power or facilities management system, EPM provides essential visibility into power conditions at a very reasonable price.” EPM software is available for download from Eaton's ePDU web site.\nPeak 10, Inc.has added generator capacity at its Raleigh data center, the company said this week. The company has added two 2.25 megawatt diesel generators, which will run in parallel with an existing 1.75 megawatt unit to give the facility a total of 6.25 megawatts of diesel generator backup. Peak 10 plans to add a third 2.25 megawatt genset and then upgrade the older generator with a fourth new 2.25 MW unit, giving Peak 10 Raleigh a total of 9 megawatts of generator backup. Peak 10 worked with Progress Energy and PowerSecure International on the project.\nActive Power, Inc. (ACPW) has announced two ordersfor a total of 16 of its 900kVA flywheel UPS units from Caterpillar (CAT). One order for 10 UPS systems is scheduled for delivery in first quarter 2009, witha  second order for six 900 kVA systems scheduled for late 2008. “These two orders exemplify the confidence the market is placing in flywheel technology and its inherent benefits of energy and space efficiency, high reliability and environmental sustainability,” said Jim Clishem, president and CEO, Active Power.",
        article_summary: "Here's a roundup of power-related news from Eaton Corp., Peak 10, Active Power (ACPW) and Caterpillar (CAT).",
        list_of_keywords: "",
        wordcloud_words: "power Eaton EPM software Peak generator Power units UPS said two megawatt total management enclosure distribution ePDUs visibility high Inc added Raleigh company diesel unit",
        wordcloud_scores: "9 5 5 4 4 4 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2",
        created_date: "2020-06-17"
    },
    {
        url: "https://www.datacenterknowledge.com/archives/2007/03/15/generator-test-saves-power-money",
        title: "Generator Test Saves Power, Money",
        body: "Is it possible to hug a tree while exercising a 2.1 Megawatt diesel generator? Not precisely. But 365 Main's flagship San Francisco data center was able to save $70,000 in power costs last year from the local utility, Pacific Gas and Electric (PG&E), by altering the procedures for monthly testing of its backup generators. PG&E highlighted 365 Main's efforts yesterday as part of a broader initiative to encourage data centers to reduce their energy use and impact upon the environment.\n\n\"365 Main is truly walking the walk in the data center segment, and we encourage other companies to follow its lead in order to protect all types of businesses from potential power outages on critical peak days and to preserve the environment,\" said Al Steubing, PG&E Account Services Director. 365 Main is a participant in PG&E's Critical Peak Pricing (CPP) program, designed to reduce stress on the good during periods of heavy power demand.\n\nAs part of its maintenance program, 365 Main tests each of its 10 Hitec generators once a month by running each of the 3,000 horsepower diesel engines for two hours. The company developed a new testing procedure that allowed it to reduce the use of load banks, the 8-foot tall heating coils traditionally used to test generators by drawing immense amounts of power. The new procedure reduced the facility's utility power consumption by as much as 12.5 percent during monthly tests, and is compliant with guidelines set by the Bay Area Air Quality Management District (BAAQMD).\n\n\"The load bank draws enough power to test a 2 megawatt generator, but it also means an additional 2 Megawatts brought through the grid during the testing phase,\" said Miles Kelly, VP of marketing for 365 Main.\n\nJ.P. Balajadia, 365 Main's Vice President of Operations, was able to devise an alternate approach. \"What JP was able to do was basically run the load being drawn by the building to the generator,  eliminating the need for the load bank,\" said Kelly. The technique is possible for data centers with exceptional equipment redudancy. An \"N+1\" approach insures that all key data center equipment has a spare on standby in case of an emergency. Kelly says 365 Main is engineered to an \"N+2\" standard, with two spare generators supporting the eight primary gensets for the facilty. Customers rely on that redundancy to remain online in case grid power is unavailable.\n\n\"The first question was, if we're testing the generator using building load, are we no longer at N+1,\" said Kelly. \"The answer is no, because you're only testing one generator and you have one more available as backup. That's the beauty of an N+2 setup.\"\n\nHundreds of PG&E business customers participate each year in the voluntary CPP program, which offers seasonal discounts to single-building customers that reduce energy usage away from peak periods between May 1 and October 31.",
        article_summary: "365 Main's San Francisco data center was able to save $70,000 in power costs last year by altering the procedures for monthly tests on its backup generators.",
        list_of_keywords: "",
        wordcloud_words: "365 Main power The generator data testing load generators reduce said Kelly center able program building possible diesel 000 year utility monthly backup part encourage",
        wordcloud_scores: "8 8 7 6 5 5 5 5 4 4 4 4 3 3 3 3 2 2 2 2 2 2 2 2 2",
        created_date: "2020-06-17"
    },
    {
        url: "https://www.datacenterknowledge.com/industry-perspectives/understanding-ups-sensitivity-and-calibration",
        title: "Understanding UPS Sensitivity and Calibration",
        body: "Randy Collier is the owner of Comp-Utility Corporation.\n\nThere are thousands more Uninterruptible Power Supply (UPS) systems under 10kVA in use than systems over 10kVA. So-called “standby,” “line-interactive” and “offline” systems are popular due to their lower price, and do not belong in mission-critical applications; therefore, they are rare in applications over 10kVA.  However, if you are stuck with an off-line system, this article discusses the problems and remedies associated with random UPS and battery failures of these systems.\n\nIn a perfect world, the power coming from your utility would be at constant voltage and frequency. In that case, these systems would remain in standby all the time and never go into alarm. In the real world, utility power fluctuates. These UPS systems need prompting before switching your critical load between conditioned battery back-up power and utility power. This is accomplished through constant monitoring of distortions in the input utility power, and when the utility power falls out of range, the UPS switches to battery power until the utility returns to acceptable limits. Every manufacturer has its own set of parameters as to when, how and how long this switching will occur, with some advertising zero clamping time to/from battery power.\n\nPower Problems\n\nProviding back-up power design would be easy if utility power was either on or off. Here are some common power problems:\n\nComplete Power Failure (Voltage = 0)\nPower Sag: Momentary Voltage drops to below an “acceptable limit”\nPower Surge: Momentary Voltage increases to above an “acceptable limit”\nUndervoltage: Voltage remains below an “acceptable limit” for an extended period of time\nOvervoltage: Voltage remains above an “acceptable limit” for an extended period of time\nLine Noise: Electromagnetic interference or Improper grounding, resulting in random disturbances\nFrequency variation: Sine wave frequency goes above or below an “acceptable limit”\nSwitching Transient: Momentary changes in voltage and current due to lightning or switching on/off of other nearby devices\nHarmonic Distortion: Transmitted by unequal loads and can result in resonance, overload, and overheating\n\nOften, these systems will switch to battery when the power appears to be on, or if running on generator. During a brown-out, for instance, the lights may be on, but the UPS may be designed to go to battery power at a low-voltage threshold. Generator power varies more than the utility, so you can expect these systems to go to battery more often while running on generator power.\n\nTrial and Error\n\nIf your UPS goes to battery often even when there seems to be available power, you may be able to adjust the sensitivity using its controls. Widening the input power parameters will allow the utility power to run your critical load without going to battery as often. Of course, this subjects your critical load to a wider set of variables, which could be problematic depending on how sensitive your critical load is. The ability to adjust the UPS input parameters varies by manufacturer and model. You can find this in the user manual or contact your manufacturer with your model and serial number for advice on fine tuning.\n\nWhen running on generator power, the UPS may not be able to track the sine wave produced by the generator, so it thinks it is protecting the load from \"dirty\" power when it shuts down the output after the batteries have expired. That could be a problem with the UPS or the generator. Sometimes sizing of the UPS and generator can be the issue. Determining that kind of fault is time consuming and costly.\n\nAnother fine point with these systems is, that at the very moment you need it is when your batteries and UPS electronics are fully tested. Unless they have a self-test, they will remain silent until you have a power problem, and then they may work as designed. If you cannot obtain the balance with these UPS systems or if your load is truly critical, you should consider “Double Conversion On-Line“ topology. These systems are designed to always run the load from the UPS' inverter, while the electric utility power keeps the battery charged. Therefore, no \"transfer\" to batteries upon a power fluctuation and, complete elimination of utility power problems. Also, the rectifier, batteries and inverter are always powered up, so even when a rare UPS fault occurs, the utility power is usually available.\n\nFinal Thoughts\n\nUsing a reactionary UPS design by default tests the metering, transfer switch, batteries and inverter at that critical moment when back up power is needed. Where have a system with adjustable input, you can make these adjustments and perhaps solve issues of multiple alarms and low battery life. Battery life is dependent on the health of the system’s rectifier and also depends on the number of discharges. So, if your battery is wearing out prematurely, or you are experiencing an abnormal number of UPS alarms, it may be a defective UPS, need an adjustment or, in problematic areas where you experience many outages, the distorted utility power is simply wearing out the batteries.\n\nOpinions expressed in the article above do not necessarily reflect the opinions of Data Center Knowledge and Informa.\n\nIndustry Perspectives is a content channel at Data Center Knowledge highlighting thought leadership in the data center arena. See our guidelines and submission process for information on participating.",
        article_summary: "So-called standby, line-interactive, and offline UPS systems are popular due to their lower price and do not belong in mission-critical applications.",
        list_of_keywords: "",
        wordcloud_words: "power UPS utility battery systems critical load acceptable generator may batteries Power time Voltage limit input will 10kVA system problems voltage need switching back manufacturer",
        wordcloud_scores: "27 17 13 12 11 7 7 6 6 6 6 5 5 5 5 4 4 3 3 3 3 3 3 3 3",
        created_date: "2020-06-17"
    },
    {
        url: "https://www.datacenterknowledge.com/archives/2012/10/01/amid-scrutiny-in-quincy-vantage-installs-cleaner-generators",
        title: "Amid Scrutiny, Cleaner Generators Arrive in Quincy",
        body: "It's been barely a week since diesel generator emissions in Quincy, Wash. made the front page of the New York Times. That might not seem to be the best moment to announce that you're installing 17 of the largest diesel generator models at a new data center in Quincy.\n\nBut Vantage Data Centers and its vendor GenAcc believe they have a different story to tell. Vantage has opted to install generators that emit far less pollution than previous units, meeting the EPA's most stringent standards for generator operation.\n\nGenAcc said this week that the Washington State Department of Ecology has certified its systems as \"Best Available Control Technology\" (BACT) for backup diesel generators in central Washington. The ecology department found that the diesel generator systems with GenAcc’s AirClarity 3000 “satisfied all requirements” and posed no significant health risk from diesel engine exhaust particulates (DEEP) to the Quincy area.\n\n“Our systems are guaranteed to reduce emissions by as much as 95 percent,” said James Richmond, GenAcc's chief executive officer. \"The Department of Ecology’s BACT certification of our technology is a confirmation of our systems’ ability to considerably reduce DEEP emissions and create a healthy environment.”\n\nData Center Cluster Draws Attention\n\nThat's become a hot topic in Quincy,  a town of 6,000 residents in central Washington state whose cheap power and cool climate has attracted major data centers for Microsoft, Yahoo, Dell, Intuit and Sabey. Each of the data centers in Quincy uses banks of generators to provide backup power in the event of a utility outage. Diesel engine exhaust is a regulated pollutant, and can be toxic in high concentrations, so state regulators routinely review permits for generators.\n\nThe growing number of diesel generators in Quincy generated debate in 2010 when Microsoft applied to add more generators for the second phase of its campus in Quincy. The state ecology department conducted an evaluation of the health risks from diesel engine exhaust particulates, and found that the Microsoft expansion was not likely to impact public health. The Pollution Control Hearings Board rebuffed a challenge from a group of citizens, including former Quincy mayor Patty Martin, who claimed that the state Department of Ecology had used faulty methodology in approving the generators.\n\nLast week the New York Times focused on the generator issue in Quincy, and Microsoft's operations in 2010, when the company's generators operated for a combined total of 3,615 hours in Quincy. That was far more than a nearby Yahoo data center, which ran its generators for just 65 hours, but below the 4,256 hours allowed by the company’s permit from the state Department of Ecology.\n\nMeeting a Higher Standard\n\nAgainst that backdrop, when Vantage Data Centers announced its project last year, it unveiled plans to equip the new facility with generators meeting EPA guidelinesto reduce hazardous emissions from backup generators. Vantage bought 63 acres of land in Quincy and plans to build up to 470,000 square feet of data center space on the property. The first phase will be a 133,000 square foot data center on 25 acres of land that has been fully-leased to a “Fortune 50 leading manufacturing and technology company,” according to Vantage. The building will have 6 megawatts of critical power, expandable to 9 megawatts.\n\nVantage plans to install 17 diesel generators on its multi-building Quincy campus, each capable of providing up to 3 megawatts of emergency backup power to maintain power to servers in the event of a utility outage. Based on its analysis, “Ecology concludes that operation of the 17 generators will not have an adverse impact on local air quality,” the department’s report said.\n\nJim Trout, president and chief executive officer of Vantage Data Centers, said controlling emissions is a priority for the company.\n\n“Quincy is rapidly becoming a key global hub for data centers and Vantage is happy to lead in the development in Quincy of some of the world’s most energy efficient, reliable and environmentally responsible data centers,” said Trout.",
        article_summary: "For its new data center in central Washington, Vantage Data Centers will install generators that emit far less pollution than previous units, meeting the EPA's most stringent standards for generator operation.",
        list_of_keywords: "",
        wordcloud_words: "Quincy generators diesel data Vantage The generator emissions said Ecology state power center Data GenAcc Department systems backup centers Microsoft company week That Centers Washington",
        wordcloud_scores: "14 13 8 8 8 7 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 3 3 3 3",
        created_date: "2020-06-17"
    },
    {
        url: "https://www.datacenterknowledge.com/machine-learning/artificial-neural-networks-proving-their-worth-enterprise",
        title: "Artificial Neural Networks Proving Their Worth in the Enterprise",
        body: "Several artificial intelligence-powered generators have had their moment in the internet sun, but few have matched the zeitgeist as well as the absurdist results of This Meme Does Not Exist.\n\nThe technology powering the generator is an artificial neural network, the same used in deepfakes. The networks are trained on large amounts of data — in this case, a dataset from Imgflip Meme Generator that draws from 100 million user-created public meme captions. To make training more efficient, 48 popular memes and 96,000 captions were used for the model.\n\nLess amusingly but more practically, artificial neural networks like the one powering this amusing meme generator also have enterprise applications.\n\n“Broadly speaking, enterprises place a lot of value on credible, fact-based content,” said Bret Greenstein, senior vice president and global markets head of AI and analytics at Cognizant.\n\nThe meme generator operates on predictive algorithms trained on patterns, Greenstein said. But the ability to use patterns to generate useful text-based content has many practical applications as well.\n\nFor example, such a system could be used to autofill job descriptions in a recruitment system. “Based on similar roles and some key words, AI could generate complete job descriptions, saving countless hours and producing more complete and consistent content,” he said. “By looking at all other job descriptions and determining which ones helped find the right candidates, the AI model could be tuned to provide the very best job descriptions which attract just the right talent.”\n\nThe technology can also be used to create content in new ways.\n\n“I see this type of AI content generation technology, as well as deepfakes, as the beginning of a future where any sort of content can be generated at will,” said Ben Ziomek, co-founder and chief product officer of Actuate.\n\nThat content likely won’t be award-worthy, Ziomek said, but the potential to create custom games or videos does have business applications.\n\n“The ramifications for entertainment, VR and simulation are endless,” he said.\n\nHowever, it’s important to remember that the technology underpinning generators like This Meme Does Not Exist is still in its early stages, Ziomek said. At the same time, that doesn’t mean there are no current applications, he said.\n\nPotential Enterprise Applications\n\nFor example, artificial neural networks like this can be used to improve relationships with customers and receive virtual feedback — appealing when social distancing makes traditional focus groups difficult to run.\n\n“Training AI on thousands of customer reviews would effectively capture the ‘voice of the customer’ for customer segmentation purposes,” Ziomek said. “Imagine an AI customer you can query at will that provides a first gut-check to marketing teams' assumptions.”\n\nOther potential applications of artificial neural networks include those using natural language processing. For example, an artificial neural network could be used to optimize marketing content and create varied copy for websites, emails and search ads, said Bruce Hogan, CEO of SoftwarePundit.\n\n“This type of optimization currently happens in a manual way, which limits the variety of options that can be produced and tested,” Hogan said.\n\nAnother potential example is catalog generation based on keywords and patterns in product descriptions, Greenstein said.\n\n“Delegating the natural language content of your business to AI might seem risky or that it could lose the human touch,” he said. “However, not all content requires the same level of judgment and experience to make it great.”\n\nAs with a lot of artificial intelligence technology currently deployed in the enterprise, neural networks thus far are best suited for reducing human busywork or adding new functionality to human-generated input, Ziomek said.\n\n“Similar to how many companies' technology helps organizations identify weapons in real time, but we still need a human to call law enforcement,” he said, “AI content generation can help companies get first stabs at content creation tasks, accelerating the initial, boring work for humans.”",
        article_summary: "An artificial neural network could be used to optimize marketing content and create varied copy for websites, emails and search ads, said Bruce Hogan, CEO of SoftwarePundit.",
        list_of_keywords: "content artificial meme network generator",
        wordcloud_words: "said content artificial technology neural used The networks applications descriptions Ziomek example job customer human well This Meme generator meme based Greenstein patterns For create",
        wordcloud_scores: "15 12 7 6 6 6 5 5 5 5 5 4 4 4 4 3 3 3 3 3 3 3 3 3 3",
        created_date: "2020-05-30"
    },
    {
        url: "https://www.networkworld.com/article/2226572/not-your-father-s-flow-export-protocol--part-2-.html#jump",
        title: "Not your Father’s Flow Export Protocol (Part 2)",
        body: "In the previous article we covered how NetFlow, IPFIX and the variety of other flow export protocols can be used to give us some application traffic visibility.  However, nothing gives as much detail as raw packet decode.  A new protocol called AppFlow may hold some promise of giving administrators the data they need while still working in a hybrid topology environment.  AppFlow, which if compatible with IPFIX, provides those application protocol details and as more vendors embrace this protocol it will become more popular.  AppFlow may strike just the right balance between providing application-layer details with the performance and flexibility of a flow export protocol.\n\nEnter AppFlow:\n\nAppFlow is a flow export protocol that not-only provides details about the TCP connection that has been made, but it also includes details about the application connection.  AppFlow provides information on the TCP connection (e.g. source/destination IP address, source/destination TCP port number, flow volume, timestamp) which is similar to NetFlow and IPFIX.  In fact, AppFlow is compatible with IPFIX.  IPFIX allows vendors to use a Vendor ID to create their own proprietary information to be exported.  The vendor can put anything they want into that export and IPFIX supports variable length fields (NetFlow does not).  AppFlow is just a form of IPFIX that is focused on application data.  Furthermore, AppFlow uses a different name so it gives people the impression that it is completely different than IPFIX.\n\nApplication-layer data is also exported in the AppFlow flow data.  AppFlow also provides information on the HTTP information and other application performance data.  AppFlow flow records include information like Round Trip Time (RTT) and protocol latency.  AppFlow records also include information about the HTTP URL, HTTP request methods and response-status codes.  AppFlow also uses Transaction ID, Connection ID and Custom IDs.\n\nThere are two components of the AppFlow architecture: a generator and a consumer.  Think of the generator like a router that might have traditionally sent NetFlow/IPFIX data.  The consumer can be thought of as being similar to the NetFlow/IPFIX collector or analyzer computer/service.  The AppFlow generator gathers up information about application flows and then sends them to the consumer for aggregation, display, analysis, and archival.  The AppFlow generator/consumer terminology is similar to the NetFlow/IPFIX exporter/collector architecture.  Different names, but the same concept and function.\n\nOne of the other benefits of AppFlow is that it can work on cloud-based systems.  It does not require a physical tap, SPAN, port mirror, or physical probe.  AppFlow also does not require an agent to be installed on a physical or virtual server.  Therefore, AppFlow may be just what administrators are looking for to give them visibility into their hybrid cloud environments.\n\nA key benefit to using AppFlow is that any standard NetFlow v9 or IPFIX collector can also be adapted to parse AppFlow data.  AppFlow transmits the flow data using UDP port 4739 with the IPFIX protocol format.  So long as the collector has the ability to parse the additional AppFlow information and analyze the data within, the AppFlow data can be analyzed by an IT administrator.  In fact, AppFlow is also sometimes referred to as IPFIX Extended.\n\nInitially there was a version 1 of the AppFlow specification, but as of May 2012 the AppFlow version 2 is now the current edition.  The AppFlow protocol is documented and maintained by the AppFlow organization on their web site appflow.org.\n\nImplementations of AppFlow:\n\nAppFlow may be somewhat new, but it is being supported by an increasing number of vendors and gaining acceptance with customers.  Also, vendors that support IPFIX today could work with AppFlow.  Following is a list of the dominant vendors that are supporting AppFlow in their products.\n\nCitrix is one of the vendors that has been the most vocal and proactive about AppFlow and has been advocating the use of the protocol.  There is broad support for AppFlow on Citrix NetScaler line of products.  A Citrix NetScaler Application Delivery Controller (ADC) can be an AppFlow generator.  AppFlow is also supported on the Citrix NetScaler 1000V ADC that is integrated with the Cisco Nexus 1000V.\n\nSplunk is the “Cookie Monster” of IT and machine data.  Splunk can be a consumer of AppFlow data and analyze the trends and pull intelligent results out of the volumes of data.  There is a Splunk App for collecting AppFlow data from Citrix NetScalers.  There is also a Splunk App for Citrix CloudBridge which would be useful in a cloud-based topology where a packet capture would be difficult if not impossible to perform.\n\nSolarwinds software can be a consumer of AppFlow data.  The Solarwinds Free Real-time AppFlow Analyzer can consume a variety of flow-export protocols and perform analysis on that data.\n\nOther companies that make software that can be AppFlow consumers are:\n\nPlixer Scrutinizer Flow Analyzer\n\nManageEngine NetFlow Analyzer (NFA)\n\nLancope StealthWatch\n\nGoliath Technologies HyperThetical\n\nCA Technologies Network Flow Analysis\n\nAs mentioned before, any IPFIX-capable system could work with AppFlow.  AppFlow and IPFIX are very much interoperable and compatible.\n\nSummary:\n\nThere are many flow-based network monitoring protocols to choose from.  Having too many choices can be confusing to consumers and lead to “analysis paralysis”.  It is helpful to know the differences and similarities between these flow-export protocols.  However, most of these protocols only send information about the higher-level characteristics and less-granular details of a connection.  Nothing beats raw packet captures, so long as the data is not encrypted, but those are not typically feasible or always available.  Packet captures are also not a solution for longer-duration analysis.\n\nIf you want to get more detailed flow information without having to break out the protocol analyzer, you should explore AppFlow and how IPFIX can be extended.  Ask your vendor if they support AppFlow and/or IPFIX and if not, when it is going to be prioritized on their product road-map.  AppFlow strikes the right balance between having a protocol that can work in many topologies, have good performance and low impact, and still give you the application visibility you require.\n\nScott",
        article_summary: "What is AppFlow and how does it differ from other flow analysis protocols?",
        list_of_keywords: "",
        wordcloud_words: "AppFlow IPFIX data flow protocol information NetFlow application export vendors The consumer Citrix protocols details There generator may provides connection collector analysis work Splunk give",
        wordcloud_scores: "48 19 17 11 11 10 8 8 6 6 6 6 6 5 5 5 5 4 4 4 4 4 4 4 3",
        created_date: "2020-06-15"
    },
    {
        url: "https://www.networkworld.com/article/2226572/not-your-father-s-flow-export-protocol--part-2-.html",
        title: "Not your Father’s Flow Export Protocol (Part 2)",
        body: "In the previous article we covered how NetFlow, IPFIX and the variety of other flow export protocols can be used to give us some application traffic visibility.  However, nothing gives as much detail as raw packet decode.  A new protocol called AppFlow may hold some promise of giving administrators the data they need while still working in a hybrid topology environment.  AppFlow, which if compatible with IPFIX, provides those application protocol details and as more vendors embrace this protocol it will become more popular.  AppFlow may strike just the right balance between providing application-layer details with the performance and flexibility of a flow export protocol.\n\nEnter AppFlow:\n\nAppFlow is a flow export protocol that not-only provides details about the TCP connection that has been made, but it also includes details about the application connection.  AppFlow provides information on the TCP connection (e.g. source/destination IP address, source/destination TCP port number, flow volume, timestamp) which is similar to NetFlow and IPFIX.  In fact, AppFlow is compatible with IPFIX.  IPFIX allows vendors to use a Vendor ID to create their own proprietary information to be exported.  The vendor can put anything they want into that export and IPFIX supports variable length fields (NetFlow does not).  AppFlow is just a form of IPFIX that is focused on application data.  Furthermore, AppFlow uses a different name so it gives people the impression that it is completely different than IPFIX.\n\nApplication-layer data is also exported in the AppFlow flow data.  AppFlow also provides information on the HTTP information and other application performance data.  AppFlow flow records include information like Round Trip Time (RTT) and protocol latency.  AppFlow records also include information about the HTTP URL, HTTP request methods and response-status codes.  AppFlow also uses Transaction ID, Connection ID and Custom IDs.\n\nThere are two components of the AppFlow architecture: a generator and a consumer.  Think of the generator like a router that might have traditionally sent NetFlow/IPFIX data.  The consumer can be thought of as being similar to the NetFlow/IPFIX collector or analyzer computer/service.  The AppFlow generator gathers up information about application flows and then sends them to the consumer for aggregation, display, analysis, and archival.  The AppFlow generator/consumer terminology is similar to the NetFlow/IPFIX exporter/collector architecture.  Different names, but the same concept and function.\n\nOne of the other benefits of AppFlow is that it can work on cloud-based systems.  It does not require a physical tap, SPAN, port mirror, or physical probe.  AppFlow also does not require an agent to be installed on a physical or virtual server.  Therefore, AppFlow may be just what administrators are looking for to give them visibility into their hybrid cloud environments.\n\nA key benefit to using AppFlow is that any standard NetFlow v9 or IPFIX collector can also be adapted to parse AppFlow data.  AppFlow transmits the flow data using UDP port 4739 with the IPFIX protocol format.  So long as the collector has the ability to parse the additional AppFlow information and analyze the data within, the AppFlow data can be analyzed by an IT administrator.  In fact, AppFlow is also sometimes referred to as IPFIX Extended.\n\nInitially there was a version 1 of the AppFlow specification, but as of May 2012 the AppFlow version 2 is now the current edition.  The AppFlow protocol is documented and maintained by the AppFlow organization on their web site appflow.org.\n\nImplementations of AppFlow:\n\nAppFlow may be somewhat new, but it is being supported by an increasing number of vendors and gaining acceptance with customers.  Also, vendors that support IPFIX today could work with AppFlow.  Following is a list of the dominant vendors that are supporting AppFlow in their products.\n\nCitrix is one of the vendors that has been the most vocal and proactive about AppFlow and has been advocating the use of the protocol.  There is broad support for AppFlow on Citrix NetScaler line of products.  A Citrix NetScaler Application Delivery Controller (ADC) can be an AppFlow generator.  AppFlow is also supported on the Citrix NetScaler 1000V ADC that is integrated with the Cisco Nexus 1000V.\n\nSplunk is the “Cookie Monster” of IT and machine data.  Splunk can be a consumer of AppFlow data and analyze the trends and pull intelligent results out of the volumes of data.  There is a Splunk App for collecting AppFlow data from Citrix NetScalers.  There is also a Splunk App for Citrix CloudBridge which would be useful in a cloud-based topology where a packet capture would be difficult if not impossible to perform.\n\nSolarwinds software can be a consumer of AppFlow data.  The Solarwinds Free Real-time AppFlow Analyzer can consume a variety of flow-export protocols and perform analysis on that data.\n\nOther companies that make software that can be AppFlow consumers are:\n\nPlixer Scrutinizer Flow Analyzer\n\nManageEngine NetFlow Analyzer (NFA)\n\nLancope StealthWatch\n\nGoliath Technologies HyperThetical\n\nCA Technologies Network Flow Analysis\n\nAs mentioned before, any IPFIX-capable system could work with AppFlow.  AppFlow and IPFIX are very much interoperable and compatible.\n\nSummary:\n\nThere are many flow-based network monitoring protocols to choose from.  Having too many choices can be confusing to consumers and lead to “analysis paralysis”.  It is helpful to know the differences and similarities between these flow-export protocols.  However, most of these protocols only send information about the higher-level characteristics and less-granular details of a connection.  Nothing beats raw packet captures, so long as the data is not encrypted, but those are not typically feasible or always available.  Packet captures are also not a solution for longer-duration analysis.\n\nIf you want to get more detailed flow information without having to break out the protocol analyzer, you should explore AppFlow and how IPFIX can be extended.  Ask your vendor if they support AppFlow and/or IPFIX and if not, when it is going to be prioritized on their product road-map.  AppFlow strikes the right balance between having a protocol that can work in many topologies, have good performance and low impact, and still give you the application visibility you require.\n\nScott",
        article_summary: "What is AppFlow and how does it differ from other flow analysis protocols?",
        list_of_keywords: "",
        wordcloud_words: "AppFlow IPFIX data flow protocol information NetFlow application export vendors The consumer Citrix protocols details There generator may provides connection collector analysis work Splunk give",
        wordcloud_scores: "48 19 17 11 11 10 8 8 6 6 6 6 6 5 5 5 5 4 4 4 4 4 4 4 3",
        created_date: "2020-06-15"
    }
]